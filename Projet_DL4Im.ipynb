{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_DL4Im.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwtP5aZHMTuajSwCe51cVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madacruz/DL4Images/blob/main/Projet_DL4Im.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Images\n",
        "\n",
        "But : réaliser et évaluer un système d'attaque adverse dans le domaine de l'identification faciale, dans un contexte closed-set (càd qu'on suppose qu'il n'y aura pas de nouvelles identités différentes du jeu d'entraînement).\n",
        "\n",
        "Tâches : \n",
        "- Identification faciale : classifier une image parmi un ensemble $n$ de photos (multi-class classification). One-to-many mapping: trouver une personne dans une BD pour l'identifier.\n",
        "    - Naive approach: softmax loss\n",
        "    - Attention ce n'est pas de la face verification qu'on souhaite faire = determiner si deux images representent la meme personne ou non (comme en TP). Ou peut etre utiliser face verification pour faire face identification ?\n",
        "- Attaques adverses\n",
        "\n",
        "\n",
        "\n",
        "Sous-tâches :\n",
        "1. Classifier : Entraînement d'un classifier ad-hoc (from scratch) et pre-trained\n",
        "2. Modèle adverse : l'attaque ne doit pas etre discernable à l'oeil nu\n",
        "3. Evaluation de la classification et du modèle adverse\n",
        "\n",
        "\n",
        "Choix du dataset :\n",
        "LFW (Labeled Faces in the Wild)\n",
        "- 13 233 images\n",
        "- 5749 personnes dont 1680 personnes avec plus de 2 photos distinctes \n",
        "- Labels = nom des personnes\n",
        "- Photos centrées sur le visage\n",
        "- RGB normalisé (pixel entre 0 et 1)\n",
        "- 250x250 original, 62x47 after preprocessing\n",
        "\n",
        "\n",
        "Questions :\n",
        "\n",
        "- Comment préparer le dataset ? Actuellement, on a un ensemble d'images labelisés, donc je peux directement séparer en train, val et test set non ?\n",
        "    - Comment gérer les personnes qui n'ont qu'une seule image ? On aura peut etre un pb du au fait qu'il y a tres peu d'images pour certaines personnes.\n",
        "    - Combien de classes ? Il y autant de classes que de personnes, mais du coup le dernier dense layer augmente lineairement avec le nombre de classes.\n",
        "- Ensuite pour le classifieur : \n",
        "    - Baseline : CNN avec softmax output \n",
        "    - Un pre-trained "
      ],
      "metadata": {
        "id": "yat1FATFDDWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXu01i1BC9jH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, as_tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.init import *\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pdb import set_trace\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage import io, transform\n",
        "from tqdm import trange, tqdm\n",
        "import csv\n",
        "import glob\n",
        "import dlib\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vJ8wRrE12AVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "MrRLEQ9_yIvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
        "!tar xvzf lfw-deepfunneled.tgz"
      ],
      "metadata": {
        "id": "DAawjh31yKvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"lfw-deepfunneled/\"\n",
        "USE_SUBSET = True"
      ],
      "metadata": {
        "id": "i_jo2o1r2UkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirs = sorted(os.listdir(PATH))\n",
        "if USE_SUBSET:\n",
        "    dirs = np.random.choice(dirs, 500, False)\n",
        "\n",
        "name_to_classid = {d: i for i, d in enumerate(dirs)}\n",
        "classid_to_name = {v: k for k, v in name_to_classid.items()}\n",
        "num_classes = len(name_to_classid)\n",
        "\n",
        "print(\"Number of classes: \", num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRjg2_Y2V0G",
        "outputId": "c6327fcc-daa7-40d5-941a-f544cf85ade7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes:  500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans chaque dossier, il y a une or plusieurs images correspondant à une personne.\n",
        "- mappings from imagepath and image id: `path_to_id` and `id_to_path`\n",
        "- mappings from class id to image ids: `classid_to_ids` and `id_to_classid`"
      ],
      "metadata": {
        "id": "Tg7mvVIKOmGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read all directories\n",
        "img_paths = {c: [PATH + subfolder + \"/\" + img\n",
        "                 for img in sorted(os.listdir(PATH + subfolder))] \n",
        "             for subfolder, c in name_to_classid.items()}\n",
        "\n",
        "# retrieve all images\n",
        "all_images_path = []\n",
        "for img_list in img_paths.values():\n",
        "    all_images_path += img_list\n",
        "\n",
        "# map to integers\n",
        "path_to_id = {v: k for k, v in enumerate(all_images_path)} \n",
        "id_to_path = {v: k for k, v in path_to_id.items()}"
      ],
      "metadata": {
        "id": "Gg8tDDDCOsww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(list(path_to_id.items())[0:13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzM0NE3qPCJ0",
        "outputId": "ad4121a3-3e6c-485a-871c-fa386d80f636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lfw-deepfunneled/Arnoldo_Aleman/Arnoldo_Aleman_0001.jpg': 1,\n",
              " 'lfw-deepfunneled/Arnoldo_Aleman/Arnoldo_Aleman_0002.jpg': 2,\n",
              " 'lfw-deepfunneled/Arnoldo_Aleman/Arnoldo_Aleman_0003.jpg': 3,\n",
              " 'lfw-deepfunneled/Arnoldo_Aleman/Arnoldo_Aleman_0004.jpg': 4,\n",
              " 'lfw-deepfunneled/Arnoldo_Aleman/Arnoldo_Aleman_0005.jpg': 5,\n",
              " 'lfw-deepfunneled/Eli_Stutsman/Eli_Stutsman_0001.jpg': 0,\n",
              " 'lfw-deepfunneled/Heather_Mills/Heather_Mills_0001.jpg': 6,\n",
              " 'lfw-deepfunneled/Heather_Mills/Heather_Mills_0002.jpg': 7,\n",
              " 'lfw-deepfunneled/Heather_Mills/Heather_Mills_0003.jpg': 8,\n",
              " 'lfw-deepfunneled/Heather_Mills/Heather_Mills_0004.jpg': 9,\n",
              " 'lfw-deepfunneled/Kevin_Stallings/Kevin_Stallings_0001.jpg': 12,\n",
              " 'lfw-deepfunneled/Patsy_Hardy/Patsy_Hardy_0001.jpg': 11,\n",
              " 'lfw-deepfunneled/Paul_Desmarais/Paul_Desmarais_0001.jpg': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build mappings between images and class\n",
        "classid_to_ids = {k: [path_to_id[path] for path in v] for k, v in img_paths.items()}\n",
        "id_to_classid = {v: c for c,imgs in classid_to_ids.items() for v in imgs}\n",
        "dict(list(id_to_classid.items())[0:13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEhC8JxAO7y3",
        "outputId": "b689cdcc-809f-45a4-b271-844f23cb2f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0,\n",
              " 1: 1,\n",
              " 2: 1,\n",
              " 3: 1,\n",
              " 4: 1,\n",
              " 5: 1,\n",
              " 6: 2,\n",
              " 7: 2,\n",
              " 8: 2,\n",
              " 9: 2,\n",
              " 10: 3,\n",
              " 11: 4,\n",
              " 12: 5}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nombre d'images par classe "
      ],
      "metadata": {
        "id": "1Yx0pdWGP8gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(classid_to_name[x], len(classid_to_ids[x]))\n",
        " for x in np.argsort([len(v) for k,v in classid_to_ids.items()])[::-1][:10]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDwSTkN3P34B",
        "outputId": "4168c1f2-7d6b-4d85-810c-306a190338e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Colin_Powell', 236),\n",
              " ('Gerhard_Schroeder', 109),\n",
              " ('Jennifer_Capriati', 42),\n",
              " ('Megawati_Sukarnoputri', 33),\n",
              " ('Ricardo_Lagos', 27),\n",
              " ('Hamid_Karzai', 22),\n",
              " ('Carlos_Menem', 21),\n",
              " ('John_Bolton', 17),\n",
              " ('Norah_Jones', 15),\n",
              " ('Kim_Clijsters', 14)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.hist([len(v) for k,v in classid_to_ids.items()], bins=range(1,10))\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "FpxNiFGtP_43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combien de classes avec 1 ou 2 images ?\n",
        "# num_images : nb de classes qui ont un nb d'images 'num_images'\n",
        "from collections import defaultdict\n",
        "\n",
        "num_classes_with_num_images = defaultdict(int)\n",
        "for k, v in list(classid_to_ids.items()):\n",
        "    num_classes_with_num_images[len(v)] += 1"
      ],
      "metadata": {
        "id": "PfbN-qa0Qiw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes_with_num_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUjPL-y_SS-V",
        "outputId": "9dd1f4eb-f2cf-41b9-9773-5bc7efedbd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {1: 342,\n",
              "             2: 75,\n",
              "             3: 22,\n",
              "             4: 18,\n",
              "             5: 8,\n",
              "             6: 7,\n",
              "             7: 6,\n",
              "             8: 3,\n",
              "             9: 3,\n",
              "             10: 1,\n",
              "             11: 3,\n",
              "             12: 1,\n",
              "             13: 1,\n",
              "             14: 1,\n",
              "             15: 1,\n",
              "             17: 1,\n",
              "             21: 1,\n",
              "             22: 1,\n",
              "             27: 1,\n",
              "             33: 1,\n",
              "             42: 1,\n",
              "             109: 1,\n",
              "             236: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Le choix de la métrique pour évaluer le classifieur est important, puisque si on choisit la mauvaise métrique, alors on choisira un modèle peu performant pour la tâche, ou même être induit en erreur par la valeur de la métrique. \n",
        "\n",
        "Choix plus complexe lorsque le jeu de données est déséquilibré. L'accuracy suppose que le jeu de données soit équilibré alors qu'ici, il y a 342 classes qui ont seulement une image, 75 qui en ont deux, etc. \n",
        "https://machinelearningmastery.com/what-is-imbalanced-classification/\n",
        "\n",
        "\n",
        "On peut jouer sur deux variables :\n",
        "- En agissant sur le jeu de données : upsampling, downsampling ? \n",
        "- Comment identifier la abonne métrique pour notre tâche ? Utiliser une autre métrique en plus de l'accuracy, eg ROC AUC (a quel point est-ce que le classifier peut distinguer les classes entre elles) https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n"
      ],
      "metadata": {
        "id": "iq4BdHxvSXsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tlJBprBgB3gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifieur\n",
        "\n",
        "La tâche que l'on souhaite résoudre ici est une tâche d'identification (*Face Identification*): étant donné l'image d'un visage d'une personne inconnue, il s'agit d'identifier le nom de cette personne en se basant sur une banque d'images de personnes déjà identifiées.\n",
        "\n",
        "__Première idée__\n",
        "\n",
        "Une première manière de formuler ce problème est de le considérer comme une tâche de classification multi-classe. Soit un ensemble de $n$ images avec au total $m$ personnes différentes (donc $m$ classes), il s'agit de classifier une image $x$ parmi ces $m$ personnes. On pourrait donc entraîner un CNN sur le jeu de données de telle sorte que le modèle puisse apprendre à extraire des caractéristiques de chaque visage, puis de les classifier avec un classifieur softmax avec $m$ classes. \n",
        "Pour implémenter cette méthode, il faut disposer de beaucoup de données pour chaque classe. Cependant, comme on l'a vu précédemment, le jeu de données est extrêmement déséquilibré (expliquer plus en détail ce problème).\n",
        "\n",
        "__Deuxième idée__\n",
        "\n",
        "Une autre manière de résoudre le problème est de partir d'une autre tâche, celle de la vérification de visages (*Face verification*). L'idée ici est de reconnaître si deux images $x_1$ et $x_2$ correspondent ou non à la même personne. Pour cela, on calcule une distance de similarité entre ces deux images, et si la valeur dépasse un certain seuil, alors on considérera que les images représentent la même personne. \n",
        "\n",
        "\n",
        "Les modèles siamois permettent de résoudre cette tâche. Deux loss :\n",
        "- Contrastive loss\n",
        "- Triplet loss\n",
        "\n",
        "Avantages des réseaux siamois :\n",
        "- Fonctionne avec peu de données (expliquer pourquoi, histoire du one-shot learning?)\n",
        "- Fonctionne avec un jeu déséquilibré (dans notre cas).\n",
        "\n",
        "Notons que notre tâche est une tâche de classification. Comment donc utiliser \n",
        "face verification -> face identification ?\n",
        "\n",
        "Une requête : soit $x$ une image qu'on souhaite classifier, on construit un ensemble de paires d'images ($x$, $x_2$) où $x2$ est une image qui représente une classe du jeu de données. Il y aura $m$ paires au total, une paire pour chaque classe/personne distincte qui existe dans le jeu. Pour la paire qui donne en sortie 1, on considèrera que $x$ appartient donc à la classe associée à $x_2$. Contrairement à la première idée où on obtient une distribution de probabilités sur les classes (celle avec la plus grande probabilité correspondra à la classe prédite), avec cette méthode, on n'aura pas de probabilités mais une valeur de distance de similarité, et selon cette valeur, on déterminera ou non si l'image appartient à la classe. \n",
        "\n",
        "Plusieurs questions :\n",
        "1. Déjà, que penses-tu de l'idée ?\n",
        "1. Comment choisir l'image représentative de chaque classe ? \n"
      ],
      "metadata": {
        "id": "RGVxaYacyLIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "TODO:\n",
        "1. Baseline : réseau siamois simple (celui qu'on a fait en TP ?)\n",
        "2. Modèle pre-trained : ArcFace \n",
        "\n",
        "___\n",
        "\n",
        "DeepFace :\n",
        "- https://scontent-cdt1-1.xx.fbcdn.net/v/t39.8562-6/240890413_887772915161178_4705912772854439762_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=QcSF_oTtLdAAX-CGlPp&_nc_ht=scontent-cdt1-1.xx&oh=00_AT9NTURwQ1zcn5JMbGswfEJ5THzcTDvzjJ4C0FYOFpb4fA&oe=62BBFCFF\n",
        "- Rectified input puis archicture : Conv, pool, conv, 3 x locally connected (?), 2 x fully connected, softmax avec k classes -> produit une distribution sur les classes\n",
        "\n",
        "ArcFace :\n",
        "- Entrée : 2 images \n",
        "- Sortie : Distance entre deux images\n",
        "\n",
        "https://towardsdatascience.com/finetune-a-facial-recognition-classifier-to-recognize-your-face-using-pytorch-d00a639d9a79"
      ],
      "metadata": {
        "id": "kmtxCtGEHe85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9EEK2jXCyMr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle adverse\n",
        "\n",
        "Voir si notre classifieur est facilement attaquable ou non ? Comment le voir ?\n"
      ],
      "metadata": {
        "id": "5U2YaQLcyM_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ngwcdUXeyOU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r3jaU-8UNICz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github"
      ],
      "metadata": {
        "id": "L6kDgS3CNIbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qmFSwzTDNJeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}